{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Функции активации и функции потерь\n",
        "\n",
        "Этот блокнот посвящен изучению ключевых компонентов, определяющих работу нейронных сетей: **функций активации** и **функций потерь**. Мы рассмотрим их свойства, применение и влияние на обучение модели с практическими примерами."
      ],
      "metadata": {
        "id": "GhhK5BTXCSpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Сгружаем датасет\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, LeakyReLU\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Загрузка данных MNIST\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Нормализация\n",
        "y_train, y_test = to_categorical(y_train), to_categorical(y_test)"
      ],
      "metadata": {
        "id": "zJ4LlWeyHxuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Часть 1: Функции активации\n",
        "Функции активации определяют, как выход каждого нейрона зависит от входных данных. Они добавляют нелинейность, что позволяет модели изучать сложные зависимости."
      ],
      "metadata": {
        "id": "Gj0CP7VWGzy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Основные функции активации\n",
        "1. **ReLU (Rectified Linear Unit)**  \n",
        "   - Одна из самых популярных функций. Возвращает ноль для отрицательных значений и линейное значение для положительных.\n",
        "   - Пример:\n",
        "   \\begin{equation}\n",
        "   f(x) = \\max(0, x)\n",
        "   \\end{equation}\n",
        "\n",
        "2. **LeakyReLU**  \n",
        "   - Модификация ReLU, позволяющая небольшие отрицательные значения. Решает проблему \"затухания нейронов\".\n",
        "   - Пример:\n",
        "   \\begin{equation}\n",
        "   f(x) =\n",
        "   \\begin{cases}\n",
        "   x, & \\text{если } x > 0 \\\\\n",
        "   \\alpha x, & \\text{если } x \\leq 0\n",
        "   \\end{cases}\n",
        "   \\end{equation}\n",
        "\n",
        "3. **Sigmoid**  \n",
        "   - Используется в выходном слое для бинарной классификации. Преобразует вход в диапазон (0, 1).\n",
        "   - Пример:\n",
        "   \\begin{equation}\n",
        "   f(x) = \\frac{1}{1 + e^{-x}}\n",
        "   \\end{equation}\n",
        "\n",
        "4. **Tanh**  \n",
        "   - Симметричный аналог `Sigmoid`. Возвращает значения в диапазоне (-1, 1).\n",
        "   - Пример:\n",
        "   \\begin{equation}\n",
        "   f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
        "   \\end{equation}\n",
        "\n",
        "5. **Softplus**  \n",
        "   - Сглаженный аналог ReLU. Возвращает положительные значения, но без резкого перехода.\n",
        "   - Пример:\n",
        "   \\begin{equation}\n",
        "   f(x) = \\ln(1 + e^x)\n",
        "   \\end{equation}\n",
        "\n",
        "6. **ELU (Exponential Linear Unit)**  \n",
        "   - Включает экспоненциальную часть для обработки отрицательных значений.\n",
        "   - Пример:\n",
        "   \\begin{equation}\n",
        "   f(x) =\n",
        "   \\begin{cases}\n",
        "   x, & \\text{если } x > 0 \\\\\n",
        "   \\alpha (e^x - 1), & \\text{если } x \\leq 0\n",
        "   \\end{cases}\n",
        "   \\end{equation}\n",
        "\n",
        "---\n",
        "\n",
        "### Пример работы с функциями активации\n",
        "Создадим модель с разными функциями активации и оценим их влияние.\n",
        "\n",
        "```python\n",
        "# Функция для обучения модели\n",
        "def train_model(activation_layer, name):\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "        Dense(128),\n",
        "        activation_layer,\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    history = model.fit(x_train, y_train, epochs=5, batch_size=32, verbose=0, validation_data=(x_test, y_test))\n",
        "    _, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f\"Функция активации: {name}, Точность на тесте: {test_accuracy:.4f}\")\n",
        "\n",
        "# Обучение моделей\n",
        "train_model(Dense(128, activation='relu'), \"ReLU\")\n",
        "train_model(Dense(128, activation='sigmoid'), \"Sigmoid\")\n",
        "train_model(Dense(128, activation='tanh'), \"Tanh\")\n",
        "train_model(Dense(128, activation='softplus'), \"Softplus\")\n",
        "train_model(Dense(128, activation='elu'), \"ELU\")\n",
        "```"
      ],
      "metadata": {
        "id": "eCZq0kkXBpCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задания:\n",
        "1. Протестируйте модели с разными функциями активации\n",
        "2. Добавьте другие функции активации (из презентации с прошлой пары) и протестируйте их\n",
        "3. Измените архитектуру модели (добавьте слои или кол-во нейронов) и протестируйте: возможно, некоторые функции активации поведут себя лучше при другой архитектуре"
      ],
      "metadata": {
        "id": "M1NvjXpuGju6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioO9x9FdA2nT"
      },
      "outputs": [],
      "source": [
        "# Code here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Часть 2: Функции потерь\n",
        "Функции потерь измеряют разницу между предсказаниями модели и истинными значениями, определяя, насколько хорошо обучается модель."
      ],
      "metadata": {
        "id": "nPKjJxVZG34q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Основные функции потерь\n",
        "1. **Mean Squared Error (MSE)**  \n",
        "   - Для задач регрессии. Измеряет среднеквадратичное отклонение между предсказаниями и истинными значениями.\n",
        "   - Пример:\n",
        "   \\begin{equation}\n",
        "   \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
        "   \\end{equation}\n",
        "\n",
        "2. **Mean Absolute Error (MAE)**  \n",
        "   - Устойчив к выбросам. Измеряет среднее абсолютное отклонение.\n",
        "   - Пример:\n",
        "   \\begin{equation}\n",
        "   \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|\n",
        "   \\end{equation}\n",
        "\n",
        "3. **Categorical Crossentropy**  \n",
        "   - Для многоклассовой классификации. Сравнивает распределения истинных и предсказанных вероятностей.\n",
        "   - Пример:\n",
        "   \\begin{equation}\n",
        "   \\text{CCE} = -\\sum_{i} y_i \\log(\\hat{y}_i)\n",
        "   \\end{equation}\n",
        "\n",
        "4. **Binary Crossentropy**  \n",
        "   - Для бинарной классификации. Использует логарифмы вероятностей для оценки ошибок.\n",
        "   - Пример:\n",
        "   \\begin{equation}\n",
        "   \\text{BCE} = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i) \\right]\n",
        "   \\end{equation}\n",
        "\n",
        "5. **Huber Loss**  \n",
        "   - Компромисс между MSE и MAE. Устойчив к выбросам.\n",
        "   - Пример:\n",
        "   \\begin{equation}\n",
        "   L_\\delta(a) =\n",
        "   \\begin{cases}\n",
        "   \\frac{1}{2}a^2, & \\text{если } |a| \\leq \\delta \\\\\n",
        "   \\delta(|a| - \\frac{1}{2}\\delta), & \\text{иначе}\n",
        "   \\end{cases}\n",
        "   \\end{equation}\n",
        "\n",
        "6. **Log-Cosh Loss**  \n",
        "   - Сглаженный аналог MAE. Хорошо справляется с выбросами.\n",
        "   - Пример:\n",
        "   \\begin{equation}\n",
        "   L(y, \\hat{y}) = \\sum \\log(\\cosh(\\hat{y} - y))\n",
        "   \\end{equation}\n",
        "\n",
        "---\n",
        "\n",
        "### Пример работы с функциями потерь\n",
        "#### Классификация\n",
        "```python\n",
        "def train_loss_classification(loss_fn, name):\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
        "    model.fit(x_train, y_train, epochs=5, batch_size=32, verbose=0, validation_data=(x_test, y_test))\n",
        "    _, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f\"Функция потерь: {name}, Точность на тесте: {test_accuracy:.4f}\")\n",
        "\n",
        "train_loss_classification('categorical_crossentropy', \"Categorical Crossentropy\")\n",
        "train_loss_classification('mse', \"Mean Squared Error\")\n",
        "```\n",
        "\n",
        "#### Регрессия (будем использовать случайные данные)\n",
        "```python\n",
        "import numpy as np\n",
        "x = np.linspace(-1, 1, 100)\n",
        "y = x**2 + np.random.normal(0, 0.1, x.shape)\n",
        "\n",
        "x_train, x_test = x[:80], x[80:]\n",
        "y_train, y_test = y[:80], y[80:]\n",
        "\n",
        "def train_loss_regression(loss_fn, name):\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(1,)),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss=loss_fn, metrics=['mae'])\n",
        "    model.fit(x_train, y_train, epochs=50, verbose=0, validation_data=(x_test, y_test))\n",
        "    test_loss = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f\"Функция потерь: {name}, Потери на тесте: {test_loss[0]:.4f}\")\n",
        "\n",
        "train_loss_regression('mse', \"Mean Squared Error\")\n",
        "train_loss_regression('mae', \"Mean Absolute Error\")\n",
        "train_loss_regression(tf.keras.losses.Huber(), \"Huber Loss\")\n",
        "train_loss_regression(tf.keras.losses.LogCosh(), \"Log-Cosh Loss\")\n",
        "```"
      ],
      "metadata": {
        "id": "CWcrCzRFCWgY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задания:\n",
        "1. Протестируйте модели с разными функциями потерь\n",
        "2. Добавьте другие функции потерь (из презентации с прошлой пары)\n",
        "3. Измените архитектуру модели (добавьте слои или кол-во нейронов) и протестируйте: возможно, некоторые функции потерь поведут себя лучше при другой архитектуре"
      ],
      "metadata": {
        "id": "ueJe_0RnG-Uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here"
      ],
      "metadata": {
        "id": "cfgtaj43CX31"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}